{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a69027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b77f7",
   "metadata": {},
   "source": [
    "# Training Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa19a3d",
   "metadata": {},
   "source": [
    "In this part we will go trough training the classifier. We start of by importing the necessary data we prepared in the last section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b19ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenfaces_features = np.load('eigenfaces_features.npy')\n",
    "emotion_labels = np.load('eigenfaces_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521db021",
   "metadata": {},
   "source": [
    "This loads the preprocessed eigenfaces features and labels from the corresponding `.npy` files using `np.load()`. Make sure the files are present in the same directory, if not so - go back to the previous notebook and run the code, which creates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53c046f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  # 0 for the default camera, you can change it if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa5b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_emotion(frame):\n",
    "    # Preprocess the frame (e.g., face detection, cropping, resizing, etc.)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale for face detection\n",
    "\n",
    "    # Perform face detection\n",
    "    face_cascade = cv2.CascadeClassifier('datasets/haarcascade_frontalface_default.xml')  # Path to face detection cascade file\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return None  # No face detected, return None or handle the case appropriately\n",
    "\n",
    "    # Extract the first face detected\n",
    "    (x, y, width, height) = faces[0]\n",
    "    face_roi = gray[y:y+height, x:x+width]  # Crop the face region from the frame\n",
    "\n",
    "    # Resize the face ROI to a fixed size\n",
    "    resized_face = cv2.resize(face_roi, (100, 100))  # Adjust the size as needed\n",
    "\n",
    "    # Extract features from the resized face\n",
    "    extracted_features = extract_features(resized_face)  # Implement your feature extraction method\n",
    "\n",
    "    # Initialize variables for emotion detection\n",
    "    min_distance = float('inf')\n",
    "    predicted_label = None\n",
    "\n",
    "    # Iterate over the eigenfaces features and compare distances\n",
    "    for i in range(eigenfaces_features.shape[0]):\n",
    "        distance = euclidean_distance(extracted_features, eigenfaces_features[i])\n",
    "\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            predicted_label = emotion_labels[i]\n",
    "\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83517fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_square(frame, x, y, width, height, label):\n",
    "    cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "    cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e707145",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Perform emotion detection on the frame\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m predicted_emotion \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_emotion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Draw a square around the face and label the emotion\u001b[39;00m\n\u001b[1;32m      8\u001b[0m draw_square(frame, x, y, width, height, predicted_emotion)\n",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mdetect_emotion\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_emotion\u001b[39m(frame):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Preprocess the frame (e.g., face detection, cropping, resizing, etc.)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert frame to grayscale for face detection\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Perform face detection\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     face_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/haarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Path to face detection cascade file\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Perform emotion detection on the frame\n",
    "    predicted_emotion = detect_emotion(frame)\n",
    "\n",
    "    # Draw a square around the face and label the emotion\n",
    "    draw_square(frame, x, y, width, height, predicted_emotion)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b0fa43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396a3fa",
   "metadata": {},
   "source": [
    "This splits the dataset into training and testing sets using the `train_test_split()` function from `scikit-learn`. It takes the eigenfaces features (`features`) and labels (`labels`) as input and splits them into training and testing sets. Here, 80% of the data is used for training, and 20% is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc67360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(feature_vector1, feature_vector2):\n",
    "    return distance.euclidean(feature_vector1, feature_vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be2c51",
   "metadata": {},
   "source": [
    "Now we create an instance of the Support Vector Machine (`SVM`) classifier with a linear kernel. It then trains the classifier using the training data (`X_train` and `y_train`) using the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34f1f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the face detection cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffddfc",
   "metadata": {},
   "source": [
    "Now we need to load the pre-trained face detection cascade classifier `XML` file using the `cv2.CascadeClassifier()` function. The `cv2.data.haarcascades` provides the path to the data directory where the pre-trained classifiers are located. The XML file used here is `haarcascade_frontalface_default.xml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5c6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the video capture\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51cebe0",
   "metadata": {},
   "source": [
    "After that we start the video capture by initializing a `VideoCapture` object cap with index `0`, which represents the default camera device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a55a2ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Convert the frame to grayscale\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Perform face detection\u001b[39;00m\n\u001b[1;32m      9\u001b[0m faces \u001b[38;5;241m=\u001b[39m face_cascade\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray, scaleFactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m, minNeighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, minSize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m30\u001b[39m))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Read the video frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Perform face detection\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Iterate over detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Extract the face region\n",
    "        face_roi = gray[y:y + h, x:x + w]\n",
    "\n",
    "        # Preprocess the face region\n",
    "        face_roi = cv2.resize(face_roi, (100, 100))\n",
    "        face_roi = face_roi / 255.0\n",
    "\n",
    "        # Flatten the face ROI\n",
    "        face_roi = face_roi.flatten()\n",
    "\n",
    "        # Perform eigenface extraction\n",
    "        eigenface = np.dot(face_roi, eigenfaces.T)\n",
    "\n",
    "        # Make emotion prediction\n",
    "        prediction = classifier.predict([eigenface])\n",
    "\n",
    "        # Map the predicted label index to the corresponding emotion label\n",
    "        predicted_emotion = emotion_labels[prediction[0]]\n",
    "\n",
    "        # Draw the predicted emotion label on the face rectangle\n",
    "        label = f\"Emotion: {predicted_emotion}\"\n",
    "        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Facial Emotion Recognition', frame)\n",
    "\n",
    "    # Exit loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84f79bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
